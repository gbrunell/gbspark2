{"paragraphs":[{"dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177339_551008856","id":"20160227-185257_756275949","dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"%md\nWhat is an RDD?\nRDD (Resilient Distributed Dataset) is main logical data unit in Spark. An RDD is distributed collection of objects. Distributed means, each RDD is divided into multiple partitions. Each of these partitions can reside in memory or stored on disk of different machines in a cluster. RDDs are immutable (Read Only) data structure. You can’t change original RDD, but you can always transform it into different RDD with all changes you want.\n\nTransformations:\nTransformations create new RDD from existing RDD like map, reduceByKey and filter we just saw. Transformations are executed on demand. That means they are computed lazily. We will see lazy evaluations more in details in next part.\n\nActions:\nActions return final results of RDD computations. Actions triggers execution using lineage graph to load the data into original RDD, carry out all intermediate transformations and return final results to Driver program or write it out to file system.\n","dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"editorMode":"ace/mode/markdown","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177339_551008856","id":"20160227-185257_1054034695","result":{"code":"SUCCESS","type":"HTML","msg":"<p>What is an RDD?\n<br  />RDD (Resilient Distributed Dataset) is main logical data unit in Spark. An RDD is distributed collection of objects. Distributed means, each RDD is divided into multiple partitions. Each of these partitions can reside in memory or stored on disk of different machines in a cluster. RDDs are immutable (Read Only) data structure. You can’t change original RDD, but you can always transform it into different RDD with all changes you want.</p>\n<p>Transformations:\n<br  />Transformations create new RDD from existing RDD like map, reduceByKey and filter we just saw. Transformations are executed on demand. That means they are computed lazily. We will see lazy evaluations more in details in next part.</p>\n<p>Actions:\n<br  />Actions return final results of RDD computations. Actions triggers execution using lineage graph to load the data into original RDD, carry out all intermediate transformations and return final results to Driver program or write it out to file system.</p>\n"},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"%pyspark\n\n# Example of a Comment and assign values to variables\n#this is a comment\n\n# assign values to variables\na = [\"Hello Spark!\"] \nb = [\"Hello\", \"Spark!\"]\nc_rdd = sc.parallelize(\"H e l l o  S p a r k !\")\nx = [1,2,3,4,5,6,7,8,9]\nlines = sc.parallelize([\"hello spark\", \"hi spark\"])\n\n#A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel.  Create of RDD is a Lazy Evaluation , meaning that Spark will not begin to execute until it sees an action. the creation of an RDD is call a \"Transformation\"\n# create Resilient Distributed Dataset (RDD) for each variable. \n\na_rdd = sc.parallelize(a)\nb_rdd = sc.parallelize(b)\nx_rdd = sc.parallelize(x)\nwordsWithMap_rdd = lines.map(lambda line: line.split(\" \"))\nwordsWithFlatMap_rdd = lines.flatMap(lambda line: line.split(\" \"))\nparallel1_rdd = sc.parallelize(range(1,10))\nparallel2_rdd = sc.parallelize(range(5,15))\n\n# Actions:  Actions return final results of RDD computations. Actions triggers execution using lineage graph to load the data into original RDD, carry out all intermediate transformations and return final results to Driver program or write it out to file system. Print is an ACTION \n\n# print number of elements in each RDD\nprint ('Number of Elements in a_rdd = ' +str(a_rdd.count()))\nprint ('Number of Elements in b_rdd = ' +str(b_rdd.count()))\nprint ('Number of Elements in c_rdd = ' +str(x_rdd.count()))\n\nprint ('Number of Elements in wordsWithMap_rdd = ' +str(wordsWithMap_rdd.count()))\nprint ('Values in wordsWithMap_rdd = ' +str(wordsWithMap_rdd.take(5)))\nprint ('Number of Elements in wordsWithFlatMap_rdd using action - count = ' +str(wordsWithFlatMap_rdd.count()))\nprint ('Values in wordsWithFlatMap_rdd using action - take = ' +str(wordsWithFlatMap_rdd.take(5)))\nprint ('Values in wordsWithFlatMap_rdd using action - first = ' +str(wordsWithFlatMap_rdd.first()))\n\n# using .map function, preform multiplication on RDDs \ny_rdd = x_rdd.map(lambda x: x * 2)\nprint ('Values in y_rdd after multiplication = ' +str(y_rdd.take(10)))\n\nc1_rdd = c_rdd.filter(lambda x: x > 5 )\nprint ('Values in c1_rdd = ' +str(c1_rdd.take(15)))\n\nwordsWithFlatMap_Reduce_rdd = wordsWithFlatMap_rdd.filter(lambda x: x <> 'spark' )\nprint ('Values in wordsWithFlatMap_Reduce_rdd = ' +str(wordsWithFlatMap_Reduce_rdd.take(15)))\n \n# Using Action .distinct create a new RDD with distinct values from 2 existing RDDs that contain some duplicate values\nparallel3_rdd = parallel1_rdd.union(parallel2_rdd).distinct()\nprint ('Values in parallel3_rdd = ' +str(parallel3_rdd.take(20)))\n\n","dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177339_551008856","id":"20160227-185257_1438411389","result":{"code":"SUCCESS","type":"TEXT","msg":"Number of Elements in a_rdd = 1\nNumber of Elements in b_rdd = 2\nNumber of Elements in c_rdd = 9\nNumber of Elements in wordsWithMap_rdd = 2\nValues in wordsWithMap_rdd = [['hello', 'spark'], ['hi', 'spark']]\nNumber of Elements in wordsWithFlatMap_rdd using action - count = 4\nValues in wordsWithFlatMap_rdd using action - take = ['hello', 'spark', 'hi', 'spark']\nValues in wordsWithFlatMap_rdd using action - first = hello\nValues in y_rdd after multiplication = [2, 4, 6, 8, 10, 12, 14, 16, 18]\nValues in c1_rdd = ['H', ' ', 'e', ' ', 'l', ' ', 'l', ' ', 'o', ' ', ' ', 'S', ' ', 'p', ' ']\nValues in wordsWithFlatMap_Reduce_rdd = ['hello', 'hi']\nValues in parallel3_rdd = [8, 1, 9, 2, 10, 11, 3, 12, 4, 5, 13, 14, 6, 7]\n"},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"title":"Example of a Comment and assignment of  variables","text":"%pyspark\n\n#this is a comment\n\n# assign values to variables\na = [\"Hello Spark!\"] \nb = [\"Hello\", \"Spark!\"]\nc_rdd = sc.parallelize(\"H e l l o  S p a r k !\")\nx = [1,2,3,4,5,6,7,8,9]\nlines = sc.parallelize([\"hello spark\", \"hi spark\"])\n","dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177340_549085111","id":"20160227-185257_1874418283","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"title":"Create of a  Resilient Distributed Dataset (RDD). the RDD is the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel.  Create of RDD is a Lazy Evaluation , meaning that Spark will not begin to execute until it sees an action. the creation of an RDD is call a \"Transformation\"","text":"%pyspark\n# create new Resilient Distributed Dataset (RDD) for each variable. \n\na_rdd = sc.parallelize(a)\nb_rdd = sc.parallelize(b)\nc_rdd = sc.parallelize(\"H e l l o  S p a r k !\")\nx_rdd = sc.parallelize(x)\nwordsWithMap_rdd = lines.map(lambda line: line.split(\" \"))\nwordsWithFlatMap_rdd = lines.flatMap(lambda line: line.split(\" \"))\nparallel1_rdd = sc.parallelize(range(1,10))\nparallel2_rdd = sc.parallelize(range(5,15))","dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177340_549085111","id":"20160227-185257_1548628419","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"text":"%md\nActions:\nActions return final results of RDD computations. Actions triggers execution using lineage graph to load the data into original RDD, carry out all intermediate transformations and return final results to Driver program or write it out to file system.\n","dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"editorMode":"ace/mode/markdown","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177340_549085111","id":"20160227-185257_2028065342","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Actions:\n<br  />Actions return final results of RDD computations. Actions triggers execution using lineage graph to load the data into original RDD, carry out all intermediate transformations and return final results to Driver program or write it out to file system.</p>\n"},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"title":"RDD Action  .map And .Reduce Examples using numeric and character data","text":"%pyspark\n# using .map function, preform multiplication on RDDs \ny_rdd = x_rdd.map(lambda x: x * 2)\nprint ('Values in y_rdd after multiplication = ' +str(y_rdd.take(10)))\n\nc1_rdd = c_rdd.filter(lambda x: x > 5 )\nprint ('Values in c1_rdd = ' +str(c1_rdd.take(15)))\n\nwordsWithFlatMap_Reduce_rdd = wordsWithFlatMap_rdd.filter(lambda x: x <> 'spark' )\nprint ('Values in wordsWithFlatMap_Reduce_rdd = ' +str(wordsWithFlatMap_Reduce_rdd.take(15)))","dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177340_549085111","id":"20160227-185257_1320722653","result":{"code":"SUCCESS","type":"TEXT","msg":"Values in y_rdd after multiplication = [2, 4, 6, 8, 10, 12, 14, 16, 18]\nValues in c1_rdd = ['H', ' ', 'e', ' ', 'l', ' ', 'l', ' ', 'o', ' ', ' ', 'S', ' ', 'p', ' ']\nValues in wordsWithFlatMap_Reduce_rdd = ['hello', 'hi']\n"},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"title":"RDD Action  Union - Return the union of two RDDs","text":"%pyspark\n# Using Action .distinct create a new RDD with distinct values from 2 existing RDDs that contain some duplicate values\nparallel3_rdd = parallel1_rdd.union(parallel2_rdd).distinct()\nprint ('Values in parallel3_rdd = ' +str(parallel3_rdd.take(20)))\n","dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177340_549085111","id":"20160227-185257_1811399011","result":{"code":"SUCCESS","type":"TEXT","msg":"Values in parallel3_rdd = [8, 1, 9, 2, 10, 11, 3, 12, 4, 5, 13, 14, 6, 7]\n"},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"title":"RDD action .count , take","text":"%pyspark\n\n# print number of elements in each RDD\n\n# print number of elements in each RDD\nprint ('Number of Elements in a_rdd = ' +str(a_rdd.count()))\nprint ('Number of Elements in b_rdd = ' +str(b_rdd.count()))\nprint ('Number of Elements in c_rdd = ' +str(x_rdd.count()))\n\nprint ('Number of Elements in wordsWithMap_rdd = ' +str(wordsWithMap_rdd.count()))\nprint ('Values in wordsWithMap_rdd = ' +str(wordsWithMap_rdd.take(5)))\nprint ('Number of Elements in wordsWithFlatMap_rdd using action - count = ' +str(wordsWithFlatMap_rdd.count()))\nprint ('Values in wordsWithFlatMap_rdd using action - take = ' +str(wordsWithFlatMap_rdd.take(5)))\nprint ('Values in wordsWithFlatMap_rdd using action - first = ' +str(wordsWithFlatMap_rdd.first()))\n","dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177341_548700363","id":"20160227-185257_1184507383","result":{"code":"SUCCESS","type":"TEXT","msg":"Number of Elements in a_rdd = 1\nNumber of Elements in b_rdd = 2\nNumber of Elements in c_rdd = 9\nNumber of Elements in wordsWithMap_rdd = 2\nValues in wordsWithMap_rdd = [['hello', 'spark'], ['hi', 'spark']]\nNumber of Elements in wordsWithFlatMap_rdd using action - count = 4\nValues in wordsWithFlatMap_rdd using action - take = ['hello', 'spark', 'hi', 'spark']\nValues in wordsWithFlatMap_rdd using action - first = hello\n"},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"dateUpdated":"Feb 27, 2016 6:52:57 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1456599177341_548700363","id":"20160227-185257_807169848","result":{"code":"SUCCESS","type":"TEXT"},"dateCreated":"Feb 27, 2016 6:52:57 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74"}],"name":"Washington DC PoT \"Hello Spark\" Simple RDD Examples v3","id":"2BD3BA8MH","angularObjects":{"2BCZ6S6TC":[],"2BDKDTYGX":[],"2BFFSBAP2":[],"2BFDPAQ4T":[]},"config":{"looknfeel":"default"},"info":{}}