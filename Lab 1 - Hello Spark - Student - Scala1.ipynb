{"nbformat_minor": 0, "cells": [{"source": "## Lab 1 - Hello Spark\nThis Lab will show you how to work with Apache Spark using Scala", "cell_type": "markdown", "metadata": {}}, {"source": "###Step 1 - Working with Spark Context\n\nStep 1 - Invoke the spark context and extract what version of the spark driver application.\n\nType<br>\nsc.version", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "//Step 1.1 - Check spark version\nsc.version", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Step 2 - Working with Resilient Distributed Datasets\n\nStep 2 - Create RDD with numbers 1 to 10,<br>\nExtract first line,<br>\nExtract first 5 lines,<br>\nCreate RDD with string \"Hello Spark\",<br>\nExtract first line.<br>\n\nType: <br>\nval x = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)<br>\nval x_nbr_rdd = sc.parallelize(x)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "//Step 2.1 - Create RDD of Numbers 1-10\nval x = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nval x_nbr_rdd = sc.parallelize(x)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Type: <br>\nx_nbr_rdd.first()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "//Step 2.2 - Extract first line\nx_nbr_rdd.first()", "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "1"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Type:<br>\nx_nbr_rdd.take(5)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "//Step 2.3 - Extract first 5 lines\nx_nbr_rdd.take(10)", "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Type:<br>\nval y = Array(\"Hello Spark!\")<br>\nval y_str_rdd = sc.parallelize(y)<br>\ny_str_rdd.first()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "//Step 2.4 - Create String RDD, Extract first line\nval y = Array(\"Hello Spark!\")\nval y_str_rdd = sc.parallelize(y)\ny_str_rdd.first()", "outputs": [{"execution_count": 6, "output_type": "execute_result", "data": {"text/plain": "Hello Spark!"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Step 3 - Count number of lines with Spark in it\nStep 3 - Pull in a spark README.md file, <br>\nConvert the file to an RDD,<br>\nCount the number of lines with the word \"Spark\" in it. <br>\n\nType:<br>\nimport sys.process._<br>\n\"rm README.md -f\" !<br>\n\"wget https://github.com/carloapp2/SparkPOT/blob/master/README.md\" !\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": "//Step 3.1 - Pull data file into workbench\nimport sys.process._\n\"rm masterInstructors-Scores.txt -f\" !\n\"wget https://github.com/carloapp2/SparkPOT/blob/master/Instructor-Scores.txt\"", "outputs": [{"execution_count": 30, "output_type": "execute_result", "data": {"text/plain": "wget https://github.com/carloapp2/SparkPOT/blob/master/Instructor-Scores.txt"}, "metadata": {}}], "metadata": {"scrolled": true, "collapsed": false, "trusted": true}}, {"source": "", "cell_type": "raw", "metadata": {}}, {"execution_count": 39, "cell_type": "code", "source": "//Step 3.2 - Create RDD from data file\nval textfile_rdd = sc.textFile(\"V:/resources/esources/asterInstructor-Scores.txt\")\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Type:<br>\nval Spark_lines = textfile_rdd.filter(lines => lines.contains(\"Spark\"))<br>\nSpark_lines.first()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "//Step 3.3 - Filter for only lines with word Spark\nval Spark_lines = textfile_rdd.filter(lines => lines.contains(\"Spark\"))\nSpark_lines.first()", "outputs": [{"execution_count": 21, "output_type": "execute_result", "data": {"text/plain": "Name: org.apache.hadoop.mapred.InvalidInputException\nMessage: Input path does not exist: file:/resources/resources/masterInstructor-Scores.txt\nStackTrace: org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:251)\norg.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:270)\norg.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)\norg.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\norg.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\nscala.Option.getOrElse(Option.scala:120)\norg.apache.spark.rdd.RDD.partitions(RDD.scala:237)\norg.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\norg.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\norg.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\nscala.Option.getOrElse(Option.scala:120)\norg.apache.spark.rdd.RDD.partitions(RDD.scala:237)\norg.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\norg.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\norg.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\nscala.Option.getOrElse(Option.scala:120)\norg.apache.spark.rdd.RDD.partitions(RDD.scala:237)\norg.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1281)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:310)\norg.apache.spark.rdd.RDD.take(RDD.scala:1276)\norg.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1316)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\norg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\norg.apache.spark.rdd.RDD.withScope(RDD.scala:310)\norg.apache.spark.rdd.RDD.first(RDD.scala:1315)\n$line174.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)\n$line174.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)\n$line174.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:32)\n$line174.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)\n$line174.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36)\n$line174.$read$$iwC$$iwC$$iwC.<init>(<console>:38)\n$line174.$read$$iwC$$iwC.<init>(<console>:40)\n$line174.$read$$iwC.<init>(<console>:42)\n$line174.$read.<init>(<console>:44)\n$line174.$read$.<init>(<console>:48)\n$line174.$read$.<clinit>(<console>)\n$line174.$eval$.<init>(<console>:7)\n$line174.$eval$.<clinit>(<console>)\n$line174.$eval.$print(<console>)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:606)\norg.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\norg.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\norg.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\norg.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\norg.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\ncom.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:296)\ncom.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:291)\ncom.ibm.spark.global.StreamState$.withStreams(StreamState.scala:80)\ncom.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\ncom.ibm.spark.interpreter.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:290)\ncom.ibm.spark.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:123)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\njava.lang.Thread.run(Thread.java:745)"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Type:<br>\nprintln(\"The file README.md has \" + Spark_lines.count() + \" of \" + textfile_rdd.count() + \" Lines with word Spark in it.\")", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "//Step 3.4 - count the number of lines\n", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Step 4 - Perform analysis on a data file\nWe have a sample file with instructors and scores. This exercise we want you to add all scores and report on results.\n\nLoad File Instructor-Scores,<br>\nMap name and scores into RDD,<br>\nAdd the 4 numbers per instructor,<br>\nPrint the total score for each instructor<br>\nPrint average score for each instructor<br>\nWho was top performer?<br>\n\nData File Format: Name,Score1,Score2,Score3,Score4<br>\nExample Line: \"Carlo,5,3,3,4\"<br>\nData File Location: https://github.com/carloapp2/SparkPOT/blob/master/Instructor-Scores.txt", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "//Step 4\n", "outputs": [], "metadata": {"scrolled": true, "collapsed": false, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Scala 2.10.4 (Spark 1.5.2)", "name": "spark", "language": "scala"}, "language_info": {"name": "scala"}}}